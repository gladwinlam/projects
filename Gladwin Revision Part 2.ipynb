{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model (name the model \"linreg\").\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "\n",
    "\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.ri\n",
    "linreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)\n",
    "\n",
    "# Compute prediction for al=2 using the predict method.\n",
    "linreg.predict(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping if want to use logistic regression. Result is either 1 or 0. So pred values can be seen as\n",
    "\n",
    "# Types 1, 2, 3 are window glass.\n",
    "# Types 5, 6, 7 are household glass.\n",
    "glass['household'] = glass.glass_type.map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})\n",
    "glass.head()\n",
    "\n",
    "# Transform household_pred to 1 or 0.\n",
    "glass['household_pred_class'] = np.where(glass.household_pred >= 0.5, 1, 0)\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict_proba(X)[0:10] #probability of 0 vs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "metrics.confusion_matrix(y_true=y_test, y_pred=logit_pred_proba > .3)\n",
    "\n",
    "#ROC/AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eada1c7e01e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Calculate RMSE for tree that splits on miles < 50,000.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RMSE:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmileage_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-eada1c7e01e5>\u001b[0m in \u001b[0;36mmileage_split\u001b[0;34m(miles)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define a function that calculates the RMSE for a given split of miles.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmileage_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlower_mileage_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiles\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mhigher_mileage_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiles\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiles\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_mileage_price\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigher_mileage_price\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a function that calculates the RMSE for a given split of miles.\n",
    "def mileage_split(miles):\n",
    "    lower_mileage_price = train[train.miles < miles].price.mean()\n",
    "    higher_mileage_price = train[train.miles >= miles].price.mean()\n",
    "    train['prediction'] = np.where(train.miles < miles, lower_mileage_price, higher_mileage_price)\n",
    "    return np.sqrt(metrics.mean_squared_error(train.price, train.prediction))\n",
    "\n",
    "# Calculate RMSE for tree that splits on miles < 50,000.\n",
    "print('RMSE:', mileage_split(50000))\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all possible mileage splits.\n",
    "mileage_range = list(range(train.miles.min(), train.miles.max(), 1000)) #start with min, increment by 1000, end with max\n",
    "RMSE = [mileage_split(miles) for miles in mileage_range]\n",
    "\n",
    "\n",
    "# Plot mileage cutpoint (x-axis) versus RMSE (y-axis).\n",
    "plt.plot(mileage_range, RMSE);\n",
    "plt.xlabel('Mileage cutpoint');\n",
    "plt.ylabel('RMSE (lower is better)');\n",
    "\n",
    "#Find the one with lowest RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Regression Tree\n",
    "Split based on reducing MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y.\n",
    "feature_cols = ['year', 'miles', 'doors', 'vtype']\n",
    "\n",
    "X = train[feature_cols]\n",
    "y = train.price\n",
    "\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor (with random_state=1).\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "treereg = DecisionTreeRegressor(random_state=1,max_depth=40)  # change the max depth to tune\n",
    "treereg\n",
    "\n",
    "# Use leave-one-out cross-validation (LOOCV) to estimate the RMSE for this model.#CV for decision tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(treereg, X, y, cv=14, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning of Max DEpth. You try different depth and see the MSE\n",
    "\n",
    "# List of values to try:\n",
    "max_depth_range = list(range(1, 8))\n",
    "\n",
    "# List to store the average RMSE for each value of max_depth:\n",
    "RMSE_scores = []\n",
    "\n",
    "# Use LOOCV with each value of max_depth.\n",
    "for depth in max_depth_range:\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    MSE_scores = cross_val_score(treereg, X, y, cv=14, scoring='neg_mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "\n",
    "    \n",
    "#Then plot graph to see RMSE agst Max Depth\n",
    "# Plot max_depth (x-axis) versus RMSE (y-axis). Find the depth with lowest MSE\n",
    "plt.plot(max_depth_range, RMSE_scores);\n",
    "plt.xlabel('max_depth');\n",
    "plt.ylabel('RMSE (lower is better)');\n",
    "\n",
    "\n",
    "\n",
    "# Sort and find the max depth to use and corresponding RMSE\n",
    "sorted(zip(RMSE_scores, max_depth_range))[0]\n",
    "\n",
    "\n",
    "\n",
    "#Then fit the tree with the optimal depth\n",
    "# max_depth=3 was best, so fit a tree using that parameter.\n",
    "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once tree model is built\n",
    "# \"Gini importance\" of each feature: the (normalized) total reduction of error brought by that feature.\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly create tree diagram \n",
    "# Create a Graphviz file.\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(treereg, out_file='./assets/tree_vehicles.dot', feature_names=feature_cols)\n",
    "\n",
    "# At the command line, run this to convert to PNG:\n",
    "#   dot -Tpng tree_vehicles.dot -o tree_vehicles.png\n",
    "\n",
    "# Or, you can drag the image below to your desktop or Powerpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUild model already, test prediction on test set to calculate performance using MSE\n",
    "\n",
    "X_test = test[feature_cols] #df\n",
    "y_test = test.price\n",
    "y_pred = treereg.predict(X_test)\n",
    "\n",
    "y_pred\n",
    "\n",
    "# Calculate RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Classification Tree\n",
    "Split based on Minimising GINI index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y.\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S']\n",
    "\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived\n",
    "\n",
    "\n",
    "# Fit a classification tree with max_depth=3 on all data.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Graphviz file.\n",
    "export_graphviz(treeclf, out_file='./assets/tree_titanic.dot', feature_names=feature_cols)\n",
    "\n",
    "# At the command line, run this to convert to PNG:\n",
    "#   dot -Tpng tree_titanic.dot -o tree_titanic.png\n",
    "\n",
    "# Or, just drag this image to your desktop or Powerpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the feature importances (the Gini index at each node).\n",
    "\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging of decision tree\n",
    "\n",
    "Reduces variance which increases accuracy. Usually for decision tree. Can be for other models\n",
    "\n",
    "Typical decision tree very high variance (ie diff model to model becos based on how it is being split.)\n",
    "\n",
    "If just want to use bagging for random forest, can just use random forest regressor. NO need bagging regressor then choose decision tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training and testing sets.\n",
    "X_train = train.iloc[:, 1:]\n",
    "y_train = train.iloc[:, 0]\n",
    "X_test = test.iloc[:, 1:]\n",
    "y_test = test.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instruct BaggingRegressor to use DecisionTreeRegressor as the \"base estimator.\"\n",
    "\n",
    "#Bagging can be applied to other methods so need to choose decision tree regressor\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagreg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=500, bootstrap=True, oob_score=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict.\n",
    "bagreg.fit(X_train, y_train)\n",
    "y_pred = bagreg.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "# Calculate RMSE.\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "#Still can generate feature importance. But unable to view visuallyGladiwn89\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classfication: M (where M<P) is the sqrt of P (full number of features)\n",
    "#Regression, m is typically chosen to be somewhere between p/3 and p.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# max_features=5 is best and n_estimators=150 is sufficiently large.# n_estimators is the number of trees you want to build\n",
    "# More the merrier but slowing to train\n",
    "rfreg = RandomForestRegressor(n_estimators=150, max_features=5, oob_score=True, random_state=1)\n",
    "rfreg.fit(X, y)\n",
    "\n",
    "# Compute feature importances.\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':rfreg.feature_importances_}).sort_values(by='importance')\n",
    "\n",
    "# Compute the out-of-bag R-squared score. HIgher the better. Means run the unseen data on the ensemble to check accuracy\n",
    "print((rfreg.oob_score_))\n",
    "\n",
    "# Find the average RMSE. #Auto take care of indivudal Y pred and Ytest\n",
    "scores = cross_val_score(rfreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection - split into train and test first before doing feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 89)\n",
    "\n",
    "\n",
    "# Fit the model on only the train data\n",
    "rfreg = RandomForestRegressor(n_estimators=150, max_features=5, oob_score=True, random_state=1)\n",
    "rfreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Set a threshold for which features to include. # include anything with importance > mean/median\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "print(SelectFromModel(rfreg, threshold='mean', prefit=True).transform(X_train).shape)\n",
    "print(SelectFromModel(rfreg, threshold='median', prefit=True).transform(X_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SelectFromModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7c9a1b6a791b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a new feature matrix that only includes important features on TEST X set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_important\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Run prediction using new X test which is X important. Then check avg RMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SelectFromModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a new feature matrix that only includes important features on TEST X set\n",
    "\n",
    "X_important =  SelectFromModel(rfreg, threshold='mean', prefit=True).transform(X_test)\n",
    "\n",
    "#Run prediction using new X test which is X important. Then check avg RMSE\n",
    "scores = cross_val_score(rfreg, X_important, y_test, cv=10, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune No of Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ba761fe7eede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Use five-fold cross-validation with each value of n_estimators (Warning: Slow!).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator_range\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrfreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mMSE_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mRMSE_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mMSE_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# List of values to try for n_estimators:\n",
    "estimator_range = list(range(10, 310, 10))\n",
    "\n",
    "# List to store the average RMSE for each value of n_estimators:\n",
    "RMSE_scores = []\n",
    "\n",
    "# Use five-fold cross-validation with each value of n_estimators (Warning: Slow!).\n",
    "for estimator in estimator_range:\n",
    "    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\n",
    "    MSE_scores = cross_val_score(rfreg, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "    \n",
    "\n",
    "# Then plot RMSE (y-axis) versus n_estimators (x-axis) and find optimal N_estimators\n",
    "\n",
    "plt.plot(estimator_range, RMSE_scores);\n",
    "\n",
    "plt.xlabel('n_estimators');\n",
    "plt.ylabel('RMSE (lower is better)');\n",
    "\n",
    "#use the Zipped and sort to find the lowest MSE and corresponding N_estimators\n",
    "# Show the best RMSE and the corresponding max n_estimators\n",
    "sorted(zip(RMSE_scores, estimator_range))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to try for max_features:\n",
    "feature_range = list(range(1, len(feature_cols)+1))\n",
    "\n",
    "# List to store the average RMSE for each value of max_features:\n",
    "RMSE_scores = []\n",
    "\n",
    "# Use 10-fold cross-validation with each value of max_features (Warning: Super slow!).\n",
    "for feature in feature_range:\n",
    "    rfreg = RandomForestRegressor(n_estimators=150, max_features=feature, random_state=1)\n",
    "    MSE_scores = cross_val_score(rfreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot max_features (x-axis) versus RMSE (y-axis).\n",
    "\n",
    "plt.plot(feature_range, RMSE_scores);\n",
    "\n",
    "plt.xlabel('max_features');\n",
    "plt.ylabel('RMSE (lower is better)');\n",
    "\n",
    "# Show the best RMSE and the corresponding max_features.\n",
    "sorted(zip(RMSE_scores, feature_range))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random search logistic regression model on the sonar dataset\n",
    "from scipy.stats import loguniform\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting REgessor\n",
    "reggbm = GradientBoostingRegressor(max_depth=3,  n_estimators=50, random_state=0)\n",
    "reggbm.fit(X,Y.values.ravel())\n",
    "Y_pred=reggbm.predict(X)\n",
    "print(r2_score(Y,Y_pred))\n",
    "\n",
    "# Calculate RMSE.\n",
    "print(np.sqrt(metrics.mean_squared_error(Y,Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ada Boost\n",
    "rega = AdaBoostRegressor( n_estimators=50, random_state=0)\n",
    "rega.fit(X,Y.values.ravel())\n",
    "Y_pred=rega.predict(X)\n",
    "print(r2_score(Y,Y_pred))\n",
    "\n",
    "# Calculate RMSE.\n",
    "print(np.sqrt(metrics.mean_squared_error(Y,Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import cluster, datasets, preprocessing, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairplots\n",
    "cols = df.columns[:-1]\n",
    "sns.pairplot(df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min max scalling. Features might have different ranges, hence normalise\n",
    "X_scaled = preprocessing.MinMaxScaler().fit_transform(df[cols])\n",
    "\n",
    "\n",
    "#Another way to scale using standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set K, build model\n",
    "k = 2\n",
    "kmeans = cluster.KMeans(n_clusters=k)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "\n",
    "#Find the labels, centroids, inertia (distance between points in same cluster. Smaller the better)\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_   #will have an array for each of the feature\n",
    "inertia = kmeans.inertia_\n",
    "\n",
    "#Find Silhouette (distance between clusters. BIgger the better)\n",
    "metrics.silhouette_score(X_scaled, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction is by allocating the labels after finding labels\n",
    "df['label'] = labels\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot diagram\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "\n",
    "plt.scatter(beer.calories, beer.alcohol, c=colors[beer.cluster], s=50);\n",
    "\n",
    "# Cluster centers, marked by \"+\"\n",
    "plt.scatter(centers.calories, centers.alcohol, linewidths=3, marker='+', s=300, c='black');\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('calories');\n",
    "plt.ylabel('alcohol');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
